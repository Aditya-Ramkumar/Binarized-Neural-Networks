{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "binarynet_mnist_mlp",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "svv6ZNwumAJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "b37b02d5-eaaa-4298-a0ac-5cea493be2f0"
      },
      "source": [
        "!pip install larq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting larq\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/10/e3888b42d76f720de84cd27bb07cd0d3c52ffe7f075dc99e7a97853fb168/larq-0.10.0-py3-none-any.whl (60kB)\n",
            "\r\u001b[K     |█████▍                          | 10kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 40kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from larq) (0.7)\n",
            "Collecting terminaltables>=3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from larq) (1.18.5)\n",
            "Building wheels for collected packages: terminaltables\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=f808b5ef84fd2f19c9d738fc68cbc84eb0e1aca250333adfcb163d87107eeed5\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built terminaltables\n",
            "Installing collected packages: terminaltables, larq\n",
            "Successfully installed larq-0.10.0 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Co1d3LqXl2HU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import larq as lq\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import LearningRateScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IgUJ6LavVgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 100\n",
        "epochs = 20\n",
        "\n",
        "# network\n",
        "num_units = 4096\n",
        "hidden_layers = 3\n",
        "use_bias = False\n",
        "\n",
        "# learning rate schedule\n",
        "lr_start = 1e-3\n",
        "lr_end = 1e-4\n",
        "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
        "\n",
        "# BN\n",
        "e = 1e-6\n",
        "momentum = 0.9\n",
        "\n",
        "# dropout\n",
        "drop_hidden = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgHMaVyul4fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "72c7fdf2-a52d-4ee8-e5ad-7584c2f4e6b7"
      },
      "source": [
        "# Import MNIST\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Flatten images\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "\n",
        "# Normalize pixel values \n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10) * 2 - 1 # -1 or 1 for hinge loss\n",
        "Y_test = np_utils.to_categorical(y_test, 10) * 2 - 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDBoLQHAnqJM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "0fe474c7-9435-44eb-836a-6def6f1cd52f"
      },
      "source": [
        "# All quantized layers except the first will use the same options\n",
        "kwargs = dict(input_quantizer=\"ste_sign\",\n",
        "              kernel_quantizer=\"ste_sign\",\n",
        "              kernel_constraint=\"weight_clip\",\n",
        "              kernel_initializer='glorot_uniform')\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# In the first layer we only quantize the weights and not the input\n",
        "model.add(lq.layers.QuantDense(num_units, \n",
        "                               kernel_quantizer=\"ste_sign\",\n",
        "                               kernel_constraint=\"weight_clip\",\n",
        "                               use_bias=use_bias,\n",
        "                               input_shape=(784,), \n",
        "                               name='dense{}'.format(1)))\n",
        "model.add(tf.keras.layers.BatchNormalization(epsilon=e, momentum=momentum, name='bn{}'.format(1)))\n",
        "model.add(tf.keras.layers.Dropout(drop_hidden, name='drop{}'.format(1)))\n",
        "\n",
        "for i in range(1, hidden_layers):\n",
        "  model.add(lq.layers.QuantDense(num_units, use_bias=True, name='dense{}'.format(i+1), **kwargs))\n",
        "  model.add(tf.keras.layers.BatchNormalization(epsilon=e, momentum=momentum, name='bn{}'.format(i+1)))\n",
        "  model.add(tf.keras.layers.Dropout(drop_hidden, name='drop{}'.format(i+1)))\n",
        "\n",
        "# L2 SVM Output layer\n",
        "model.add(tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2()))\n",
        "model.add(tf.keras.layers.Activation('linear'))\n",
        "\n",
        "\n",
        "lq.models.summary(model)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+sequential_7 stats---------------------------------------------------------------------------+\n",
            "| Layer         Input prec.     Outputs   # 1-bit  # 32-bit   Memory  1-bit MACs  32-bit MACs |\n",
            "|                     (bit)                   x 1       x 1     (kB)                          |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "| dense1                  -  (-1, 4096)   3211264         0   392.00           0      3211264 |\n",
            "| bn1                     -  (-1, 4096)         0      8192    32.00           0            0 |\n",
            "| drop1                   -  (-1, 4096)         0         0        0           ?            ? |\n",
            "| dense2                  1  (-1, 4096)  16777216      4096  2064.00    16777216            0 |\n",
            "| bn2                     -  (-1, 4096)         0      8192    32.00           0            0 |\n",
            "| drop2                   -  (-1, 4096)         0         0        0           ?            ? |\n",
            "| dense3                  1  (-1, 4096)  16777216      4096  2064.00    16777216            0 |\n",
            "| bn3                     -  (-1, 4096)         0      8192    32.00           0            0 |\n",
            "| drop3                   -  (-1, 4096)         0         0        0           ?            ? |\n",
            "| dense_4                 -    (-1, 10)         0     40970   160.04           0        40960 |\n",
            "| activation_4            -    (-1, 10)         0         0        0           ?            ? |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "| Total                                  36765696     73738  4776.04    33554432      3252224 |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "+sequential_7 summary--------------------------+\n",
            "| Total params                      36.8 M     |\n",
            "| Trainable params                  36.8 M     |\n",
            "| Non-trainable params              24.6 k     |\n",
            "| Model size                        4.66 MiB   |\n",
            "| Model size (8-bit FP weights)     4.45 MiB   |\n",
            "| Float-32 Equivalent               140.53 MiB |\n",
            "| Compression Ratio of Memory       0.03       |\n",
            "| Number of MACs                    36.8 M     |\n",
            "| Ratio of MACs that are binarized  0.9116     |\n",
            "+----------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNoVUU2mi6mT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "2ec9bfaa-ad89-4514-9ab9-d1a592763693"
      },
      "source": [
        "model.compile(tf.keras.optimizers.Adam(lr=0.01, decay=0.0001),\n",
        "              loss='squared_hinge',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
        "\n",
        "print(f\"Test accuracy {test_acc * 100:.2f} %\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "600/600 [==============================] - 524s 873ms/step - loss: 1.3835 - accuracy: 0.8050\n",
            "Epoch 2/20\n",
            "600/600 [==============================] - 546s 909ms/step - loss: 0.3315 - accuracy: 0.8883\n",
            "Epoch 3/20\n",
            "600/600 [==============================] - 524s 873ms/step - loss: 0.1025 - accuracy: 0.9351\n",
            "Epoch 4/20\n",
            "600/600 [==============================] - 529s 882ms/step - loss: 0.1060 - accuracy: 0.9417\n",
            "Epoch 5/20\n",
            "600/600 [==============================] - 529s 882ms/step - loss: 0.1118 - accuracy: 0.9414\n",
            "Epoch 6/20\n",
            "600/600 [==============================] - 511s 851ms/step - loss: 0.0735 - accuracy: 0.9531\n",
            "Epoch 7/20\n",
            "600/600 [==============================] - 513s 855ms/step - loss: 0.0653 - accuracy: 0.9595\n",
            "Epoch 8/20\n",
            "600/600 [==============================] - 511s 852ms/step - loss: 0.0591 - accuracy: 0.9626\n",
            "Epoch 9/20\n",
            "600/600 [==============================] - 512s 853ms/step - loss: 0.0540 - accuracy: 0.9654\n",
            "Epoch 10/20\n",
            "600/600 [==============================] - 515s 859ms/step - loss: 0.0510 - accuracy: 0.9675\n",
            "Epoch 11/20\n",
            "600/600 [==============================] - 518s 864ms/step - loss: 0.0486 - accuracy: 0.9692\n",
            "Epoch 12/20\n",
            "600/600 [==============================] - 515s 859ms/step - loss: 0.0501 - accuracy: 0.9690\n",
            "Epoch 13/20\n",
            "600/600 [==============================] - 510s 850ms/step - loss: 0.0512 - accuracy: 0.9700\n",
            "Epoch 14/20\n",
            "600/600 [==============================] - 510s 849ms/step - loss: 0.0515 - accuracy: 0.9699\n",
            "Epoch 15/20\n",
            "600/600 [==============================] - 506s 844ms/step - loss: 0.0485 - accuracy: 0.9723\n",
            "Epoch 16/20\n",
            "600/600 [==============================] - 505s 842ms/step - loss: 0.0435 - accuracy: 0.9763\n",
            "Epoch 17/20\n",
            "600/600 [==============================] - 504s 840ms/step - loss: 0.0423 - accuracy: 0.9766\n",
            "Epoch 18/20\n",
            "600/600 [==============================] - 506s 843ms/step - loss: 0.0381 - accuracy: 0.9790\n",
            "Epoch 19/20\n",
            "600/600 [==============================] - 503s 838ms/step - loss: 0.0362 - accuracy: 0.9807\n",
            "Epoch 20/20\n",
            "600/600 [==============================] - 505s 841ms/step - loss: 0.0356 - accuracy: 0.9816\n",
            "313/313 [==============================] - 63s 201ms/step - loss: 0.0943 - accuracy: 0.9710\n",
            "Test accuracy 97.10 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}